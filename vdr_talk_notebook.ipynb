{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d008ee",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/visual_document_retrieval_in_fiftyone_talk/blob/main/vdr_talk_notebook.ipynb)\n",
    "\n",
    "### The Challenge:\n",
    "\n",
    "- 1,134 vision papers at NeurIPS\n",
    "- 3 days to explore\n",
    "- Which 30-40 papers should you prioritize?\n",
    "\n",
    "\n",
    "### The Workflow:\n",
    "\n",
    "#### Step 1: Visualize the Landscape\n",
    "\n",
    "- Load dataset ‚Üí Compute embeddings ‚Üí Generate UMAP\n",
    "\n",
    "- See what research clusters emerge: diffusion, transformers, 3D, video\n",
    "\n",
    "- Understand: What's hot? What's emerging? Where do areas overlap?\n",
    "\n",
    "#### Step 2: Find Core Interests\n",
    "\n",
    "- Semantic seach based on your interests\n",
    "\n",
    "- Lasso entire clusters: Tag interesting papers as 'core_interest'\n",
    "\n",
    "- Filter by presentation type: Oral vs Poster\n",
    "\n",
    "#### Step 3: Discover Through Semantic Similarity\n",
    "\n",
    "- Find papers with similar research niches\n",
    "\n",
    "- Find papers similar to ones you already like\n",
    "\n",
    "- Discover cross-domain connections\n",
    "\n",
    "#### Step 4: Identify Novel Work\n",
    "\n",
    "- Sort by representativeness (low scores = outliers)\n",
    "\n",
    "- Papers that don't fit existing categories\n",
    "\n",
    "- Potential breakthroughs or ambitious cross-domain work\n",
    "\n",
    "#### Step 5: Build Your Schedule\n",
    "\n",
    "- Core papers + Adjacent + Outliers\n",
    "\n",
    "- Filter to oral presentations ‚Üí 15 must-attend\n",
    "\n",
    "- Export personalized conference guide\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Let's install our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone torch transformers pillow umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ab40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/illuin-tech/colpali.git@vbert#egg=colpali-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaea68c",
   "metadata": {},
   "source": [
    "Let's install some plugins to help us along the way. Run the following in your terminal:\n",
    "\n",
    "1. `fiftyone plugins download https://github.com/jacobmarks/keyword-search-plugin`\n",
    "\n",
    "2. `fiftyone plugins download https://github.com/harpreetsahota204/caption-viewer`\n",
    "\n",
    "3. `fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3327a",
   "metadata": {},
   "source": [
    "Begin by loading the [Visual AI at NeurIPS 2025 dataset](https://huggingface.co/datasets/Voxel51/visual_ai_at_neurips2025), which is hosted on Hugging Face. \n",
    "\n",
    "This dataset contains NeurIPS 2025 accepted papers focused on computer vision and related fields, enriched with arXiv metadata and first-page images. \n",
    "\n",
    "It includes papers from multiple vision-related categories including Computer Vision (cs.CV), Multimedia (cs.MM), Image and Video Processing (eess.IV), Graphics (cs.GR), and Robotics (cs.RO). \n",
    "\n",
    "Each entry includes paper metadata, abstracts, author information, and a high-resolution (500 DPI) PNG image of the paper's first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "# Load the dataset\n",
    "# Note: other available arguments include 'max_samples', etc\n",
    "dataset = load_from_hub(\"Voxel51/visual_ai_at_neurips2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2d307",
   "metadata": {},
   "source": [
    "Let's call the Dataset.\n",
    "\n",
    "When you \"call the dataset\" in FiftyOne‚Äîsuch as by printing it with `print(dataset)`, you get a summary of the dataset's structure and contents. \n",
    "\n",
    "This includes information like the number of samples, available fields, and possibly a preview of the first or last sample. \n",
    "\n",
    "This is a useful way to inspect your dataset after loading or creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9e5d0",
   "metadata": {},
   "source": [
    "We've got 1,134 papers here. Let's understand what we're working with by looking at the first sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5990d97",
   "metadata": {},
   "source": [
    "We can get a sense of the distribution of `arxiv_category` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3300f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs.RO': 94, 'cs.MM': 2, 'cs.CV': 996, 'cs.GR': 22, 'eess.IV': 20}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"arxiv_category.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e0e1f",
   "metadata": {},
   "source": [
    "Now, let's [map these category labels](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.map_labels) to something more human readable. We're doing this because, towards the end of this notebook, we'll use visual document retrieval model to perform zero shot classification of the document images.\n",
    "\n",
    "Begin by [cloning the sample field](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.clone_sample_field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43912f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.clone_sample_field(\"arxiv_category\", \"arxiv_category_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6795bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"cs.CV\": \"Computer Vision\",\n",
    "    \"cs.MM\": \"Multimedia\",\n",
    "    \"eess.IV\": \"Image and Video Processing\",\n",
    "    \"cs.GR\": \"Graphics\",\n",
    "    \"cs.RO\": \"Robotics\",\n",
    "}\n",
    "\n",
    "view = dataset.map_labels(\"arxiv_category_mapped\", mapping)\n",
    "view.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559c010",
   "metadata": {},
   "source": [
    "And we can verify this worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad177da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Graphics': 22,\n",
       " 'Robotics': 94,\n",
       " 'Multimedia': 2,\n",
       " 'Image and Video Processing': 20,\n",
       " 'Computer Vision': 996}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"arxiv_category_mapped.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6085f",
   "metadata": {},
   "source": [
    "Now, launch the app and do some initial exploration of the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf223910",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ea4f8",
   "metadata": {},
   "source": [
    "### Setup the model\n",
    "\n",
    "For this demo, we're going to use [ColModernVBert](https://docs.voxel51.com/plugins/plugins_ecosystem/colmodernvbert.html).\n",
    "\n",
    "ColModernVBert is a multi-vector vision-language model built on the ModernVBert architecture that generates ColBERT-style embeddings for both images and text. \n",
    "\n",
    "Unlike single-vector models that compress entire images into a single representation, ColModernVBert produces multiple 128-dimensional vectors per input, enabling fine-grained matching between specific image regions and text tokens.\n",
    "\n",
    "I'm using it here because it's lightweight (250M parameters), and even without a GPU you can run the model and explore later on.\n",
    "\n",
    "##### üìå Some other models you may want to check out later:\n",
    "\n",
    "| Model | Parameters | Output | Key Features | Good For |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **[`nomic-embed-multimodal`]((https://docs.voxel51.com/plugins/plugins_ecosystem/nomic_embed_multimodal.html))** | 3B and 7B | Multi-dimensional vectors | Available in two sizes | Multimodal embedding tasks|\n",
    "| **[`bimodernvbert`](https://docs.voxel51.com/plugins/plugins_ecosystem/bimodernvbert.html)** | 250M | 768-dim single vectors | Runs fast on CPU - about 7x faster than comparable models | When you need speed and don't have a GPU |\n",
    "| **[`colmodernvbert`](https://docs.voxel51.com/plugins/plugins_ecosystem/colmodernvbert.html)** | 250M | Multi-vectors (ColBERT-style) | Same base as bimodernvbert, matches models 10x its size on vidore benchmarks | Fine-grained document matching with maxsim scoring|\n",
    "| **[`jina-embeddings-v4`](https://docs.voxel51.com/plugins/plugins_ecosystem/jina_embeddings_v4.html)** | 3.8B | 2048-dim single-vector or multi-vector | Supports 30+ languages, task-specific LoRA adapters for retrieval, text-matching, and code | Multilingual document retrieval across different tasks|\n",
    "| **[`colqwen2-5-v0-2`](https://docs.voxel51.com/plugins/plugins_ecosystem/colqwen2_5_v0_2.html)** | qwen2.5-vl-3B | Multi-vectors | Preserves aspect ratios, dynamic resolution up to 768 patches, token pooling keeps ~97.8% accuracy | Document layouts where aspect ratio matters |\n",
    "| **[`colpali-v1-3`](https://docs.voxel51.com/plugins/plugins_ecosystem/colpali_v1_3.html)** | paligemma-3B | Multi-vector late interaction | Original model that showed visual doc retrieval could beat OCR pipelines | Baseline multi-vector retrieval, well-tested |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aadd3d",
   "metadata": {},
   "source": [
    "### Register the Zoo Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this repository as a remote zoo model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/colmodernvbert\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c32f3",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffc4c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ColModernVBert model\n",
    "model = foz.load_zoo_model(\n",
    "    \"ModernVBERT/colmodernvbert\",\n",
    "    pooling_strategy=\"mean\"  # or \"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a989337",
   "metadata": {},
   "source": [
    "### Compute embeddings\n",
    "\n",
    "Now, we can use the [`compute_embeddings`](https://docs.voxel51.com/api/fiftyone.core.models.html#fiftyone.core.models.compute_embeddings) method on our entire document collection. \n",
    "\n",
    "This is a one-time operation that turns each document into a vector representation that captures its visual and semantic meaning.\n",
    "\n",
    "##### What's Happening Under the Hood?\n",
    "\n",
    "- Each image is processed by ColModernVBERT ‚Üí generates ~884 vectors (128-dim each)\n",
    "\n",
    "- These multi-vectors are pooled (using max/mean pooling) ‚Üí single 128-dim embedding\n",
    "\n",
    "- The pooled embeddings are stored as fields of the FiftyOne dataset\n",
    "\n",
    "- This gives us the best of both worlds: fine-grained multi-vector representation compressed into efficient single vectors for retrieval.\n",
    "\n",
    "**Note:** This took ~1.5 hours on my Mac M3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd41436",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=model,\n",
    "    embeddings_field=\"colmodernvbert_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220bf7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Check embedding dimensions\n",
    "print(dataset.first()['colmodernvbert_embeddings'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe230a",
   "metadata": {},
   "source": [
    "#### ‚ÑπÔ∏è Let me save you sometime\n",
    "\n",
    "If you want to skip waiting for the model run, you can download a dataset with these embeddings (and the zero-shot classifications we do later) and follow along with the rest of the notebook.\n",
    "\n",
    "This is how you can download it:\n",
    "\n",
    "```python\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\"harpreetsahota/visual_ai_at_neurips2025_colmodernvbert\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207631f",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Once we have embeddings, we can visualize them. This is where magic happens.\n",
    "\n",
    "The [`compute_visualization`](https://docs.voxel51.com/api/fiftyone.brain.visualization.html#fiftyone.brain.visualization.visualize) method in FiftyOne will create a 2D visualization of our document embeddings using UMAP (Uniform Manifold Approximation and Projection).\n",
    "\n",
    "This will help us:\n",
    "\n",
    "- See how documents cluster in the embedding space\n",
    "- Identify similar documents visually\n",
    "- Understand the semantic structure of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bbce9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Thu Nov  6 10:32:30 2025 Construct fuzzy simplicial set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  6 10:32:31 2025 Finding Nearest Neighbors\n",
      "Thu Nov  6 10:32:31 2025 Finished Nearest Neighbor Search\n",
      "Thu Nov  6 10:32:31 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf38bbaf26904fe2a96b1a8e0e03f33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Thu Nov  6 10:32:31 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"colmodernvbert_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"colmodernvbert_viz\",\n",
    "    num_dims=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92636d67",
   "metadata": {},
   "source": [
    "When you open the [embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel) in the FiftyOne App, you'll see a bunch of dots.\n",
    "\n",
    "Each dot is a document. Documents that are visually and semantically similar are placed close together. \n",
    "\n",
    "And without us telling it anything about document types or categories, natural clusters emerge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07146f99",
   "metadata": {},
   "source": [
    "### Build Similarity Index\n",
    "\n",
    "Now let's use the [`compute_similarity`](https://docs.voxel51.com/api/fiftyone.brain.similarity.html#fiftyone-brain-similarity) method to build a similarity index. This is where visual document retrieval becomes incredibly powerful for research discovery.\n",
    "\n",
    "This index enables three types of search that transform how you explore 1,134 papers:\n",
    "\n",
    "1. Text-to-image search\n",
    "\n",
    "    Natural language queries like \"diffusion models for medical imaging\" or \"papers with architecture diagrams\" find relevant content in abstracts and visuals.\n",
    "\n",
    "2. Image-to-image search\n",
    "\n",
    "    Click any paper to find others with similar diagrams, notation, or presentation styles.\n",
    "\n",
    "3. Cross-domain discovery\n",
    "    Find connections keywords miss‚Äîlike papers sharing architectural approaches across different fields or citing similar foundational work.\n",
    "\n",
    "Search by semantic meaning, visual structure, and notation style simultaneously. This could help in discovering papers traditional keyword search wouldn't find.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model= \"ModernVBERT/colmodernvbert\",\n",
    "    embeddings_field=\"colmodernvbert_embeddings\",\n",
    "    brain_key=\"colmodernvbert_sim\",\n",
    "    model_kwargs={\"pooling_strategy\": \"mean\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889ce13",
   "metadata": {},
   "source": [
    "You'll see how to do all this in the App as well, but you can perform semantic similarity search with text queries\n",
    "\n",
    "For this query, we'll retrieve the top 3 most similar documents.\n",
    "\n",
    "[`sort_by_similarity`](https://docs.voxel51.com/api/fiftyone.brain.similarity.html) method returns a `fiftyone.core.view.DatasetView` containing the 3 most similar samples to your text query. \n",
    "\n",
    "You can use this view directly in various ways:\n",
    "\n",
    "- Display it in the FiftyOne App: `session.view = sims`\n",
    "- Iterate over the samples: `for sample in sims: ...`\n",
    "- Apply additional view operations: [`sims.match(...)`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match)\n",
    "- Access the samples: `sims.first()`, [`sims.take(n)`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.take), etc.\n",
    "\n",
    "If you want to persist this view for later use, you can [save it to your dataset](https://docs.voxel51.com/user_guide/using_views.html#similarity-views) by tagging the samples or storing the similarity scores in a field using the `dist_field` parameter:\n",
    "\n",
    "This will store the similarity distance for each sample in a field called \"similarity_score\" on the samples themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfa1ff20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b3fce4b7c94549b025ebebae14414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sims = text_img_index.sort_by_similarity(\n",
    "    [\"visual document retrieval\"],\n",
    "    k=3,\n",
    "    dist_field=\"similarity_score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18f7e4",
   "metadata": {},
   "source": [
    "### Compute Uniqueness\n",
    "\n",
    "With the embeddings we can [compute a uniqueness score](https://docs.voxel51.com/brain.html#brain-image-uniqueness) for every paper - how different is it from all the others?\n",
    "\n",
    "**`compute_uniqueness`** assigns each paper a uniqueness score (0-1) based on how different it is from the rest of the conference.\n",
    "\n",
    "**Low scores (0.1-0.3)**: Papers in heavily researched areas with incremental variations. Read one representative, skip the rest.\n",
    "\n",
    "**High scores (0.7-0.9)**: Novel approaches that don't fit existing categories. These are your potential breakthrough papers.\n",
    "\n",
    "**Use this to** prioritize unique contributions over the 10th variation of the same idea, and discover papers that don't fit the mainstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f9f7a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing uniqueness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.uniqueness:Computing uniqueness...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniqueness computation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.uniqueness:Uniqueness computation complete\n"
     ]
    }
   ],
   "source": [
    "results = fob.compute_uniqueness(\n",
    "    dataset,\n",
    "    embeddings=\"colmodernvbert_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5367599",
   "metadata": {},
   "source": [
    "### Near Duplicates\n",
    "\n",
    "**[`compute_near_duplicates`](https://docs.voxel51.com/brain.html#near-duplicates)** finds groups of very similar papers by comparing embeddings against a threshold. At a large conference like NeurIPS, this helps you:\n",
    "\n",
    "- **Avoid redundancy**: Don't read multiple papers that are essentially the same approach with minor variations\n",
    "\n",
    "- **Identify research trends**: Find groups of papers from different teams converging on similar solutions\n",
    "\n",
    "- **Efficient scheduling**: If 3 papers in your queue are near-duplicates, attend one talk and skim the others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e73ad8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing duplicate samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.similarity:Computing duplicate samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates computation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.similarity:Duplicates computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "dup_index = fob.compute_near_duplicates(\n",
    "    dataset,\n",
    "    embeddings=\"colmodernvbert_embeddings\",\n",
    "    threshold=0.02,  # Adjust as needed for your data/model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07334578",
   "metadata": {},
   "source": [
    "This creates two saved views on your dataset:\n",
    "\n",
    "- **`near duplicates`**: All papers that are very similar to one or more other papers. These are your \"related work clusters\" - papers you should compare side-by-side to understand subtle differences in approach.\n",
    "\n",
    "- **`representatives of near duplicates`**: One representative from each cluster of similar papers. Read these first to understand each approach, then decide if the variations are worth diving into.\n",
    "\n",
    "**Example use case**: You find 5 papers about diffusion models for medical imaging that cluster tightly together. Read the representative paper to understand the core approach, then skim the others to see what each team did differently - architecture tweaks, different datasets, alternative loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164656c",
   "metadata": {},
   "source": [
    "##### ü§î What's the difference between computing uniqueness and near duplicates?\n",
    "\n",
    "| Method | `compute_near_duplicates` | `compute_uniqueness` |\n",
    "|:---|:---|:---|\n",
    "| **Purpose** | Detects potential near-duplicate samples | Scores how unique each sample is |\n",
    "| **Goal** | Find groups of very similar samples | Rank all samples by uniqueness |\n",
    "| **How it works** | Measures distance between embeddings; samples below threshold are duplicates | Analyzes similarity distribution across entire dataset |\n",
    "| **Output** | `SimilarityIndex` object with duplicate IDs and neighbor mappings | Adds scalar `uniqueness` field (0-1) to each sample |\n",
    "| **Score meaning** | Binary: duplicate or not | Higher = more unique, Lower = more similar to others \n",
    "| **Primary use case** | Dataset cleaning (remove redundant data) | Sample selection (choose diverse samples for annotation/training) |\n",
    "| **Requires threshold** | Yes | No |\n",
    "\n",
    "\n",
    "**Key difference:** One finds duplicates to remove; the other ranks samples to find the most diverse ones to keep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32fb45",
   "metadata": {},
   "source": [
    "### Compute Representativeness\n",
    "\n",
    "This finds [the most prototypical](https://docs.voxel51.com/brain.html#image-representativeness) papers in your dataset.\n",
    "\n",
    "##### One way to interpret these scores\n",
    "\n",
    "**High representativeness scores** identify mainstream papers - the ones that best represent each research cluster. These are your \"survey the field\" papers that show what's typical in diffusion models, vision transformers, or 3D reconstruction. If you want to understand the current state of a research area, start here.\n",
    "\n",
    "**Low representativeness scores** identify outliers and boundary papers - the ones that don't fit neatly into existing clusters. These are often the most interesting: novel approaches combining multiple areas, cross-domain applications, or genuinely new methods. These are your \"potential breakthrough\" papers.\n",
    "\n",
    "For conference planning: read the high-representativeness papers to get oriented in each area, then explore the low-representativeness papers to find cutting-edge work that might define future directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4047cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing representativeness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.representativeness:Computing representativeness...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing clusters for 1134 embeddings; this may take awhile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.representativeness:Computing clusters for 1134 embeddings; this may take awhile...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representativeness computation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.representativeness:Representativeness computation complete\n"
     ]
    }
   ],
   "source": [
    "# Compute representativeness scores\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    representativeness_field=\"colmodernvbert_represent\",\n",
    "    method=\"cluster-center\",\n",
    "    embeddings=\"colmodernvbert_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3992ab",
   "metadata": {},
   "source": [
    "### Zero-shot Classification\n",
    "\n",
    "We can even use this model to perform zero-shot classification. In this example, we will see how well this model can classify the arXiv category of the paper.\n",
    "\n",
    "Let's get a list of the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33dcbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_categories = dataset.distinct(\"arxiv_category_mapped.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78bd38c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Vision',\n",
       " 'Graphics',\n",
       " 'Image and Video Processing',\n",
       " 'Multimedia',\n",
       " 'Robotics']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779ccbf",
   "metadata": {},
   "source": [
    "Then we can use the [apply_model]() method of the dataset. \n",
    "\n",
    "Notice the `‚Å†text_prompt` argument. This customizes how class names are embedded for comparison with images. It's a template (e.g., \"A research paper from the arXiv category of \") that's combined with each class label to form text inputs like \"A research paper from the arXiv category of Robotics\" or \"A research paper from the arXiv category of Graphics\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd165677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "model.text_prompt=\"A research paper from the arXiv category of \"\n",
    "model.classes=arxiv_categories\n",
    "\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"arxiv_category_predictions\"\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca9900",
   "metadata": {},
   "source": [
    "We can also see how well it does with unmapped categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50bf5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmapped_arxiv_categories = dataset.distinct(\"arxiv_category.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a3d0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs.CV', 'cs.GR', 'cs.MM', 'cs.RO', 'eess.IV']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmapped_arxiv_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63508b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown key 'model_kwargs'. The supported keys are ['filepath']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtext_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA research paper from the arXiv category of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m=\u001b[39munmapped_arxiv_categories\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munmapped_arxiv_category_predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooling_strategy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/collections.py:3563\u001b[0m, in \u001b[0;36mSampleCollection.apply_model\u001b[0;34m(self, model, label_field, confidence_thresh, store_logits, batch_size, num_workers, skip_failures, output_dir, rel_dir, progress, **kwargs)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_model\u001b[39m(\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3505\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3516\u001b[0m ):\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the model to the samples in the collection.\u001b[39;00m\n\u001b[1;32m   3518\u001b[0m \n\u001b[1;32m   3519\u001b[0m \u001b[38;5;124;03m    This method supports all of the following cases:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3561\u001b[0m \u001b[38;5;124;03m            to the underlying inference implementation\u001b[39;00m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3563\u001b[0m     \u001b[43mfomo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfidence_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/models.py:258\u001b[0m, in \u001b[0;36mapply_model\u001b[0;34m(samples, model, label_field, confidence_thresh, store_logits, batch_size, num_workers, skip_failures, output_dir, rel_dir, progress, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _apply_image_model_to_frames_single(\n\u001b[1;32m    248\u001b[0m         samples,\n\u001b[1;32m    249\u001b[0m         model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m         progress,\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_data_loader:\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_image_model_data_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfidence_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfield_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _apply_image_model_batch(\n\u001b[1;32m    273\u001b[0m         samples,\n\u001b[1;32m    274\u001b[0m         model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m         progress,\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/models.py:445\u001b[0m, in \u001b[0;36m_apply_image_model_data_loader\u001b[0;34m(samples, model, label_field, confidence_thresh, batch_size, num_workers, skip_failures, filename_maker, progress, field_mapping)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_image_model_data_loader\u001b[39m(\n\u001b[1;32m    432\u001b[0m     samples,\n\u001b[1;32m    433\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     field_mapping,\n\u001b[1;32m    442\u001b[0m ):\n\u001b[1;32m    443\u001b[0m     needs_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(model, SamplesMixin)\n\u001b[0;32m--> 445\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m \u001b[43m_make_data_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfield_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_samples:\n\u001b[1;32m    455\u001b[0m         fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mneeds_fields\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/models.py:856\u001b[0m, in \u001b[0;36m_make_data_loader\u001b[0;34m(samples, model, batch_size, num_workers, skip_failures, field_mapping)\u001b[0m\n\u001b[1;32m    853\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, SupportsGetItem):\n\u001b[0;32m--> 856\u001b[0m     get_item \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_get_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto_torch(get_item, skip_failures\u001b[38;5;241m=\u001b[39mskip_failures)\n\u001b[1;32m    858\u001b[0m     worker_init_fn \u001b[38;5;241m=\u001b[39m fout\u001b[38;5;241m.\u001b[39mFiftyOneTorchDataset\u001b[38;5;241m.\u001b[39mworker_init\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/utils/torch.py:734\u001b[0m, in \u001b[0;36mTorchImageModel.build_get_item\u001b[0;34m(self, field_mapping)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_get_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImageGetItem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43musing_half_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_using_half_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfield_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/utils/torch.py:622\u001b[0m, in \u001b[0;36mImageGetItem.__init__\u001b[0;34m(self, field_mapping, transform, raw_inputs, using_half_precision, use_numpy, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    615\u001b[0m     field_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    621\u001b[0m ):\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfield_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_inputs \u001b[38;5;241m=\u001b[39m raw_inputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/utils/torch.py:284\u001b[0m, in \u001b[0;36mGetItem.__init__\u001b[0;34m(self, field_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_mapping \u001b[38;5;241m=\u001b[39m {k: k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys}\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# This updates `_field_mapping` via the attribute setter\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield_mapping\u001b[49m \u001b[38;5;241m=\u001b[39m field_mapping\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/utils/torch.py:322\u001b[0m, in \u001b[0;36mGetItem.field_mapping\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys:\n\u001b[0;32m--> 322\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. The supported keys are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_mapping[k] \u001b[38;5;241m=\u001b[39m v\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown key 'model_kwargs'. The supported keys are ['filepath']"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "model.text_prompt=\"A research paper from the arXiv category of \"\n",
    "model.classes=unmapped_arxiv_categories\n",
    "\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"unmapped_arxiv_category_predictions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e2458",
   "metadata": {},
   "source": [
    "### Evaluate Classifications\n",
    "\n",
    "FiftyOne has a nice [evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) that you can use to assess how well a model performs.\n",
    "\n",
    "By default, `evaluate_classifications` will treat your classifications as generic multiclass predictions, and it will evaluate each prediction by directly comparing its label to the associated ground truth prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e5c253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"arxiv_category_predictions\",\n",
    "    gt_field=\"arxiv_category_mapped\",\n",
    "    eval_key=\"eval_simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd582ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee738d53",
   "metadata": {},
   "source": [
    "### Now let's go to the App and explore in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc5d23e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://localhost:5151/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75a2b0",
   "metadata": {},
   "source": [
    "When you started this talk, you had documents. \n",
    "\n",
    "Maybe you had metadata: filenames, dates, categories. \n",
    "\n",
    "But you didn't really know your data.\n",
    "\n",
    "Now? You can see it.\n",
    "\n",
    "You can see how documents cluster. \n",
    "\n",
    "You can find the duplicates inflating your dataset. \n",
    "\n",
    "You can discover connections between documents that keywords would miss. \n",
    "\n",
    "You can identify the prototypical examples and the edge cases. \n",
    "\n",
    "You can search for documents with similar diagrams, similar table structures, similar visual patterns.\n",
    "\n",
    "You transformed from 'I have documents' to 'I understand my dataset.'\n",
    "\n",
    "You've seen what's possible. How do you actually start using this on your own documents?\n",
    "\n",
    "The workflow is simple. \n",
    "\n",
    "Four steps:\n",
    "\n",
    "One: Embed. Load your documents, pick a model, compute embeddings. BiModernVBERT is a great starting point because it runs on CPU and is fast enough for most use cases.\n",
    "\n",
    "Two: Visualize. Generate a UMAP plot and look at your data. What clusters form? Where are the outliers? This 30-second view tells you more than hours of manual sampling.\n",
    "\n",
    "Three: Explore. Use similarity search, uniqueness, representativeness - whatever insights you need. Find duplicates. Discover similar documents. Identify prototypes.\n",
    "\n",
    "Four: Understand. You now know what you have, what you're missing, and what's unusual. You can make informed decisions about what to annotate, what to use for training, what to use for testing.\n",
    "\n",
    "Take 100 documents from your current project. Run this code. Look at the visualization. I guarantee you'll see something you didn't know about your dataset:\n",
    "\n",
    "- Clusters you didn't expect\n",
    "- Outliers that surprise you\n",
    "- Duplicates you didn't know existed\n",
    "- Connections keywords can't find\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
