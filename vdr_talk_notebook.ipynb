{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d008ee",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/visual_document_retrieval_in_fiftyone_talk/blob/main/vdr_talk_notebook.ipynb)\n",
    "\n",
    "### The Challenge:\n",
    "\n",
    "- 1,134 vision papers at NeurIPS\n",
    "- 3 days to explore\n",
    "- Which 30-40 papers should you prioritize?\n",
    "\n",
    "\n",
    "### The Workflow:\n",
    "\n",
    "#### Step 1: Visualize the Landscape\n",
    "\n",
    "- Load dataset ‚Üí Compute embeddings ‚Üí Generate UMAP\n",
    "\n",
    "- See what research clusters emerge: diffusion, transformers, 3D, video\n",
    "\n",
    "- Understand: What's hot? What's emerging? Where do areas overlap?\n",
    "\n",
    "#### Step 2: Find Core Interests\n",
    "\n",
    "- Semantic seach based on your interests\n",
    "\n",
    "- Lasso entire clusters: Tag interesting papers as 'core_interest'\n",
    "\n",
    "- Filter by presentation type: Oral vs Poster\n",
    "\n",
    "#### Step 3: Discover Through Semantic Similarity\n",
    "\n",
    "- Find papers with similar research niches\n",
    "\n",
    "- Find papers similar to ones you already like\n",
    "\n",
    "- Discover cross-domain connections\n",
    "\n",
    "#### Step 4: Identify Novel Work\n",
    "\n",
    "- Sort by representativeness (low scores = outliers)\n",
    "\n",
    "- Papers that don't fit existing categories\n",
    "\n",
    "- Potential breakthroughs or ambitious cross-domain work\n",
    "\n",
    "#### Step 5: Build Your Schedule\n",
    "\n",
    "- Core papers + Adjacent + Outliers\n",
    "\n",
    "- Filter to oral presentations ‚Üí 15 must-attend\n",
    "\n",
    "- Export personalized conference guide\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Let's install our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone torch transformers pillow umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ab40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/illuin-tech/colpali.git@vbert#egg=colpali-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaea68c",
   "metadata": {},
   "source": [
    "Let's install some plugins to help us along the way. Run the following in your terminal:\n",
    "\n",
    "1. `fiftyone plugins download https://github.com/jacobmarks/keyword-search-plugin`\n",
    "\n",
    "2. `fiftyone plugins download https://github.com/harpreetsahota204/caption-viewer`\n",
    "\n",
    "3. `fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3327a",
   "metadata": {},
   "source": [
    "Begin by loading the [Visual AI at NeurIPS 2025 dataset](https://huggingface.co/datasets/Voxel51/visual_ai_at_neurips2025), which is hosted on Hugging Face. \n",
    "\n",
    "This dataset contains NeurIPS 2025 accepted papers focused on computer vision and related fields, enriched with arXiv metadata and first-page images. \n",
    "\n",
    "It includes papers from multiple vision-related categories including Computer Vision (cs.CV), Multimedia (cs.MM), Image and Video Processing (eess.IV), Graphics (cs.GR), and Robotics (cs.RO). \n",
    "\n",
    "Each entry includes paper metadata, abstracts, author information, and a high-resolution (500 DPI) PNG image of the paper's first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "# Load the dataset\n",
    "# Note: other available arguments include 'max_samples', etc\n",
    "dataset = load_from_hub(\"Voxel51/visual_ai_at_neurips2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2d307",
   "metadata": {},
   "source": [
    "Let's call the Dataset.\n",
    "\n",
    "When you \"call the dataset\" in FiftyOne‚Äîsuch as by printing it with `print(dataset)`, you get a summary of the dataset's structure and contents. \n",
    "\n",
    "This includes information like the number of samples, available fields, and possibly a preview of the first or last sample. \n",
    "\n",
    "This is a useful way to inspect your dataset after loading or creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ac7f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        neurips-2025-vision-papers\n",
      "Media type:  image\n",
      "Num samples: 1134\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    type:             fiftyone.core.fields.StringField\n",
      "    name:             fiftyone.core.fields.StringField\n",
      "    virtualsite_url:  fiftyone.core.fields.StringField\n",
      "    abstract:         fiftyone.core.fields.StringField\n",
      "    arxiv_id:         fiftyone.core.fields.StringField\n",
      "    arxiv_authors:    fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    arxiv_category:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9e5d0",
   "metadata": {},
   "source": [
    "We've got 1,134 papers here. Let's understand what we're working with by looking at the first sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70a6746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '690c5772c27d837eef1b53a3',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/Users/harpreetsahota/workspace/random-hackings/neurips_2025_papers/neurips_cv_images/2510.11296v2.png',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': 1173920,\n",
       "        'mime_type': 'image/png',\n",
       "        'width': 4250,\n",
       "        'height': 5500,\n",
       "        'num_channels': 3,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 11, 6, 8, 8, 18, 592000),\n",
       "    'last_modified_at': datetime.datetime(2025, 11, 6, 8, 8, 18, 891000),\n",
       "    'type': 'Poster',\n",
       "    'name': '$\\\\Delta \\\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization',\n",
       "    'virtualsite_url': 'https://neurips.cc/virtual/2025/poster/116579',\n",
       "    'abstract': \"Recent approaches for vision-language models (VLMs) have shown remarkable success in achieving fast downstream adaptation. When applied to real-world downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data and out-of-distribution (OOD) data. The OOD datasets often include both covariate shifts (e.g., known classes with changes in image styles) and semantic shifts (e.g., test-time unseen classes). This highlights the importance of improving VLMs' generalization ability to covariate-shifted OOD data, while effectively detecting open-set semantic-shifted OOD classes. In this paper, inspired by the substantial energy change observed in closed-set data when re-aligning vision-language modalities‚Äîspecifically by directly reducing the maximum cosine similarity to a low value‚Äîwe introduce a novel OOD score, named $\\\\Delta\\\\mathrm{Energy}$. $\\\\Delta\\\\mathrm{Energy}$ significantly outperforms the vanilla energy-based OOD score and provides a more reliable approach for OOD detection. Furthermore, $\\\\Delta\\\\mathrm{Energy}$ can simultaneously improve OOD generalization under covariate shifts, which is achieved by lower-bound maximization for $\\\\Delta\\\\mathrm{Energy}$ (termed EBM). EBM is theoretically proven to not only enhance OOD detection but also yields a domain-consistent Hessian, which serves as a strong indicator for OOD generalization. Based on this finding, we developed a unified fine-tuning framework that allows for improving VLMs' robustness in both OOD generalization and OOD detection. Extensive experiments on challenging OOD detection and generalization benchmarks demonstrate the superiority of our method, outperforming recent approaches by 10\\\\%‚Äì25\\\\% in AUROC.\",\n",
       "    'arxiv_id': '2510.11296v2',\n",
       "    'arxiv_authors': [\n",
       "        'Lin Zhu',\n",
       "        'Yifeng Yang',\n",
       "        'Xinbing Wang',\n",
       "        'Qinying Gu',\n",
       "        'Nanyang Ye',\n",
       "    ],\n",
       "    'arxiv_category': <Classification: {\n",
       "        'id': '690c5772c27d837eef1b4f35',\n",
       "        'tags': [],\n",
       "        'label': 'cs.CV',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5990d97",
   "metadata": {},
   "source": [
    "We can get a sense of the distribution of `arxiv_category` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3300f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs.MM': 2, 'cs.GR': 22, 'cs.RO': 94, 'eess.IV': 20, 'cs.CV': 996}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"arxiv_category.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e0e1f",
   "metadata": {},
   "source": [
    "Now, let's [map these category labels](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.map_labels) to something more human readable. We're doing this because, towards the end of this notebook, we'll use visual document retrieval model to perform zero shot classification of the document images.\n",
    "\n",
    "Begin by [cloning the sample field](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.clone_sample_field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43912f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.clone_sample_field(\"arxiv_category\", \"arxiv_category_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6795bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"cs.CV\": \"Computer Vision\",\n",
    "    \"cs.MM\": \"Multimedia\",\n",
    "    \"eess.IV\": \"Image and Video Processing\",\n",
    "    \"cs.GR\": \"Graphics\",\n",
    "    \"cs.RO\": \"Robotics\",\n",
    "}\n",
    "\n",
    "view = dataset.map_labels(\"arxiv_category_mapped\", mapping)\n",
    "view.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559c010",
   "metadata": {},
   "source": [
    "And we can verify this worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad177da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Graphics': 22,\n",
       " 'Image and Video Processing': 20,\n",
       " 'Multimedia': 2,\n",
       " 'Computer Vision': 996,\n",
       " 'Robotics': 94}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"arxiv_category_mapped.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6085f",
   "metadata": {},
   "source": [
    "Now, launch the app and do some initial exploration of the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf223910",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ea4f8",
   "metadata": {},
   "source": [
    "### Setup the model\n",
    "\n",
    "For this demo, we're going to use [ColModernVBert](https://docs.voxel51.com/plugins/plugins_ecosystem/colmodernvbert.html).\n",
    "\n",
    "ColModernVBert is a multi-vector vision-language model built on the ModernVBert architecture that generates ColBERT-style embeddings for both images and text. \n",
    "\n",
    "Unlike single-vector models that compress entire images into a single representation, ColModernVBert produces multiple 128-dimensional vectors per input, enabling fine-grained matching between specific image regions and text tokens.\n",
    "\n",
    "I'm using it here because it's lightweight (250M parameters), and even without a GPU you can run the model and explore later on.\n",
    "\n",
    "##### üìå Some other models you may want to check out later:\n",
    "\n",
    "| Model | Parameters | Output | Key Features | Good For |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **[`nomic-embed-multimodal`]((https://docs.voxel51.com/plugins/plugins_ecosystem/nomic_embed_multimodal.html))** | 3B and 7B | Multi-dimensional vectors | Available in two sizes | Multimodal embedding tasks|\n",
    "| **[`bimodernvbert`](https://docs.voxel51.com/plugins/plugins_ecosystem/bimodernvbert.html)** | 250M | 768-dim single vectors | Runs fast on CPU - about 7x faster than comparable models | When you need speed and don't have a GPU |\n",
    "| **[`colmodernvbert`](https://docs.voxel51.com/plugins/plugins_ecosystem/colmodernvbert.html)** | 250M | Multi-vectors (ColBERT-style) | Same base as bimodernvbert, matches models 10x its size on vidore benchmarks | Fine-grained document matching with maxsim scoring|\n",
    "| **[`jina-embeddings-v4`](https://docs.voxel51.com/plugins/plugins_ecosystem/jina_embeddings_v4.html)** | 3.8B | 2048-dim single-vector or multi-vector | Supports 30+ languages, task-specific LoRA adapters for retrieval, text-matching, and code | Multilingual document retrieval across different tasks|\n",
    "| **[`colqwen2-5-v0-2`](https://docs.voxel51.com/plugins/plugins_ecosystem/colqwen2_5_v0_2.html)** | qwen2.5-vl-3B | Multi-vectors | Preserves aspect ratios, dynamic resolution up to 768 patches, token pooling keeps ~97.8% accuracy | Document layouts where aspect ratio matters |\n",
    "| **[`colpali-v1-3`](https://docs.voxel51.com/plugins/plugins_ecosystem/colpali_v1_3.html)** | paligemma-3B | Multi-vector late interaction | Original model that showed visual doc retrieval could beat OCR pipelines | Baseline multi-vector retrieval, well-tested |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aadd3d",
   "metadata": {},
   "source": [
    "### Register the Zoo Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d9b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/harpreetsahota204/colmodernvbert...\n",
      "   69.4Mb [714.8ms elapsed, ? remaining, 97.0Mb/s]  \n",
      "Overwriting existing model source '/Users/harpreetsahota/fiftyone/__models__/colmodernvbert'\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this repository as a remote zoo model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/colmodernvbert\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c32f3",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ColModernVBert model\n",
    "model = foz.load_zoo_model(\n",
    "    \"ModernVBERT/colmodernvbert\",\n",
    "    pooling_strategy=\"mean\"  # or \"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a989337",
   "metadata": {},
   "source": [
    "### Compute embeddings\n",
    "\n",
    "Now, we can use the [`compute_embeddings`](https://docs.voxel51.com/api/fiftyone.core.models.html#fiftyone.core.models.compute_embeddings) method on our entire document collection. \n",
    "\n",
    "This is a one-time operation that turns each document into a vector representation that captures its visual and semantic meaning.\n",
    "\n",
    "##### What's Happening Under the Hood?\n",
    "\n",
    "- Each image is processed by ColModernVBERT ‚Üí generates ~884 vectors (128-dim each)\n",
    "\n",
    "- These multi-vectors are pooled (using max/mean pooling) ‚Üí single 128-dim embedding\n",
    "\n",
    "- The pooled embeddings are stored as fields of the FiftyOne dataset\n",
    "\n",
    "- This gives us the best of both worlds: fine-grained multi-vector representation compressed into efficient single vectors for retrieval.\n",
    "\n",
    "**Note:** This took ~1.5 hours on my Mac M3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd41436",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=model,\n",
    "    embeddings_field=\"colmodernvbert_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220bf7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embedding dimensions\n",
    "print(dataset.first()['colmodernvbert_embeddings'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe230a",
   "metadata": {},
   "source": [
    "#### ‚ÑπÔ∏è Let me save you sometime\n",
    "\n",
    "If you want to skip waiting for the model run, you can download a dataset with these embeddings (and the zero-shot classifications we do later) and follow along with the rest of the notebook.\n",
    "\n",
    "This is how you can download it:\n",
    "\n",
    "```python\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\"harpreetsahota/visual_ai_at_neurips2025_colmodernvbert\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207631f",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Once we have embeddings, we can visualize them. This is where magic happens.\n",
    "\n",
    "The [`compute_visualization`](https://docs.voxel51.com/api/fiftyone.brain.visualization.html#fiftyone.brain.visualization.visualize) method in FiftyOne will create a 2D visualization of our document embeddings using UMAP (Uniform Manifold Approximation and Projection).\n",
    "\n",
    "This will help us:\n",
    "\n",
    "- See how documents cluster in the embedding space\n",
    "- Identify similar documents visually\n",
    "- Understand the semantic structure of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbce9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"colmodernbert_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"colmodernbert_viz\",\n",
    "    num_dims=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92636d67",
   "metadata": {},
   "source": [
    "When you open the [embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel) in the FiftyOne App, you'll see a bunch of dots.\n",
    "\n",
    "Each dot is a document. Documents that are visually and semantically similar are placed close together. \n",
    "\n",
    "And without us telling it anything about document types or categories, natural clusters emerge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07146f99",
   "metadata": {},
   "source": [
    "### Build Similarity Index\n",
    "\n",
    "Now let's use the [`compute_similarity`](https://docs.voxel51.com/api/fiftyone.brain.similarity.html#fiftyone-brain-similarity) method to build a similarity index. This is where visual document retrieval becomes incredibly powerful for research discovery.\n",
    "\n",
    "This index enables three types of search that transform how you explore 1,134 papers:\n",
    "\n",
    "1. Text-to-image search\n",
    "\n",
    "    Natural language queries like \"diffusion models for medical imaging\" or \"papers with architecture diagrams\" find relevant content in abstracts and visuals.\n",
    "\n",
    "2. Image-to-image search\n",
    "\n",
    "    Click any paper to find others with similar diagrams, notation, or presentation styles.\n",
    "\n",
    "3. Cross-domain discovery\n",
    "    Find connections keywords miss‚Äîlike papers sharing architectural approaches across different fields or citing similar foundational work.\n",
    "\n",
    "Search by semantic meaning, visual structure, and notation style simultaneously. This could help in discovering papers traditional keyword search wouldn't find.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model= \"ModernVBERT/colmodernvbert\",\n",
    "    embeddings_field=\"colmodernbert_embeddings\",\n",
    "    brain_key=\"colmodernbert_sim\",\n",
    "    model_kwargs={\"pooling_strategy\": \"mean\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889ce13",
   "metadata": {},
   "source": [
    "You'll see how to do all this in the App as well, but you can perform semantic similarity search with text queries\n",
    "\n",
    "For this query, we'll retrieve the top 3 most similar documents.\n",
    "\n",
    "[`sort_by_similarity`](https://docs.voxel51.com/api/fiftyone.brain.similarity.html) method returns a `fiftyone.core.view.DatasetView` containing the 3 most similar samples to your text query. \n",
    "\n",
    "You can use this view directly in various ways:\n",
    "\n",
    "- Display it in the FiftyOne App: `session.view = sims`\n",
    "- Iterate over the samples: `for sample in sims: ...`\n",
    "- Apply additional view operations: [`sims.match(...)`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match)\n",
    "- Access the samples: `sims.first()`, [`sims.take(n)`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.take), etc.\n",
    "\n",
    "If you want to persist this view for later use, you can [save it to your dataset](https://docs.voxel51.com/user_guide/using_views.html#similarity-views) by tagging the samples or storing the similarity scores in a field using the `dist_field` parameter:\n",
    "\n",
    "This will store the similarity distance for each sample in a field called \"similarity_score\" on the samples themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = text_img_index.sort_by_similarity(\n",
    "    \"paper about visual document retrieval\",\n",
    "    k=3,\n",
    "    dist_field=\"similarity_score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18f7e4",
   "metadata": {},
   "source": [
    "### Compute Uniqueness\n",
    "\n",
    "With the embeddings we can [compute a uniqueness score](https://docs.voxel51.com/brain.html#brain-image-uniqueness) for every paper - how different is it from all the others?\n",
    "\n",
    "**`compute_uniqueness`** assigns each paper a uniqueness score (0-1) based on how different it is from the rest of the conference.\n",
    "\n",
    "**Low scores (0.1-0.3)**: Papers in heavily researched areas with incremental variations. Read one representative, skip the rest.\n",
    "\n",
    "**High scores (0.7-0.9)**: Novel approaches that don't fit existing categories. These are your potential breakthrough papers.\n",
    "\n",
    "**Use this to** prioritize unique contributions over the 10th variation of the same idea, and discover papers that don't fit the mainstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_uniqueness(\n",
    "    dataset,\n",
    "    embeddings=\"colmodernvbert_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5367599",
   "metadata": {},
   "source": [
    "### Near Duplicates\n",
    "\n",
    "**[`compute_near_duplicates`](https://docs.voxel51.com/brain.html#near-duplicates)** finds groups of very similar papers by comparing embeddings against a threshold. At a large conference like NeurIPS, this helps you:\n",
    "\n",
    "- **Avoid redundancy**: Don't read multiple papers that are essentially the same approach with minor variations\n",
    "\n",
    "- **Identify research trends**: Find groups of papers from different teams converging on similar solutions\n",
    "\n",
    "- **Efficient scheduling**: If 3 papers in your queue are near-duplicates, attend one talk and skim the others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ad8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "dup_index = fob.compute_near_duplicates(\n",
    "    dataset,\n",
    "    embeddings=\"colmodernbert_embeddings\",\n",
    "    threshold=0.02,  # Adjust as needed for your data/model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07334578",
   "metadata": {},
   "source": [
    "This creates two saved views on your dataset:\n",
    "\n",
    "- **`near duplicates`**: All papers that are very similar to one or more other papers. These are your \"related work clusters\" - papers you should compare side-by-side to understand subtle differences in approach.\n",
    "\n",
    "- **`representatives of near duplicates`**: One representative from each cluster of similar papers. Read these first to understand each approach, then decide if the variations are worth diving into.\n",
    "\n",
    "**Example use case**: You find 5 papers about diffusion models for medical imaging that cluster tightly together. Read the representative paper to understand the core approach, then skim the others to see what each team did differently - architecture tweaks, different datasets, alternative loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164656c",
   "metadata": {},
   "source": [
    "##### ü§î What's the difference between computing uniqueness and near duplicates?\n",
    "\n",
    "| Method | `compute_near_duplicates` | `compute_uniqueness` |\n",
    "|:---|:---|:---|\n",
    "| **Purpose** | Detects potential near-duplicate samples | Scores how unique each sample is |\n",
    "| **Goal** | Find groups of very similar samples | Rank all samples by uniqueness |\n",
    "| **How it works** | Measures distance between embeddings; samples below threshold are duplicates | Analyzes similarity distribution across entire dataset |\n",
    "| **Output** | `SimilarityIndex` object with duplicate IDs and neighbor mappings | Adds scalar `uniqueness` field (0-1) to each sample |\n",
    "| **Score meaning** | Binary: duplicate or not | Higher = more unique, Lower = more similar to others \n",
    "| **Primary use case** | Dataset cleaning (remove redundant data) | Sample selection (choose diverse samples for annotation/training) |\n",
    "| **Requires threshold** | Yes | No |\n",
    "\n",
    "\n",
    "**Key difference:** One finds duplicates to remove; the other ranks samples to find the most diverse ones to keep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32fb45",
   "metadata": {},
   "source": [
    "### Compute Representativeness\n",
    "\n",
    "This finds [the most prototypical](https://docs.voxel51.com/brain.html#image-representativeness) papers in your dataset.\n",
    "\n",
    "##### One way to interpret these scores\n",
    "\n",
    "**High representativeness scores** identify mainstream papers - the ones that best represent each research cluster. These are your \"survey the field\" papers that show what's typical in diffusion models, vision transformers, or 3D reconstruction. If you want to understand the current state of a research area, start here.\n",
    "\n",
    "**Low representativeness scores** identify outliers and boundary papers - the ones that don't fit neatly into existing clusters. These are often the most interesting: novel approaches combining multiple areas, cross-domain applications, or genuinely new methods. These are your \"potential breakthrough\" papers.\n",
    "\n",
    "For conference planning: read the high-representativeness papers to get oriented in each area, then explore the low-representativeness papers to find cutting-edge work that might define future directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute representativeness scores\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    representativeness_field=\"colmodernvbert_represent\",\n",
    "    method=\"cluster-center\",\n",
    "    embeddings=\"colmodernvbert_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3992ab",
   "metadata": {},
   "source": [
    "### Zero-shot Classification\n",
    "\n",
    "We can even use this model to perform zero-shot classification. In this example, we will see how well this model can classify the arXiv category of the paper.\n",
    "\n",
    "Let's get a list of the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_categories = dataset.distinct(\"arxiv_category_mapped.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779ccbf",
   "metadata": {},
   "source": [
    "Then we can use the [apply_model]() method of the dataset. \n",
    "\n",
    "Notice the `‚Å†text_prompt` argument. This customizes how class names are embedded for comparison with images. It's a template (e.g., \"A research paper from the arXiv category of \") that's combined with each class label to form text inputs like \"A research paper from the arXiv category of Robotics\" or \"A research paper from the arXiv category of Graphics\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd165677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "model.text_prompt=\"A research paper from the arXiv category of \"\n",
    "model.classes=arxiv_categories\n",
    "\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"arxiv_category_predictions\"\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e2458",
   "metadata": {},
   "source": [
    "### Evaluate Classifications\n",
    "\n",
    "FiftyOne has a nice [evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) that you can use to assess how well a model performs.\n",
    "\n",
    "By default, `evaluate_classifications` will treat your classifications as generic multiclass predictions, and it will evaluate each prediction by directly comparing its label to the associated ground truth prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"arxiv_category_predictions\",\n",
    "    gt_field=\"arxiv_category_mapped\",\n",
    "    eval_key=\"eval_simple\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee738d53",
   "metadata": {},
   "source": [
    "### Now let's go to the App and explore in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75a2b0",
   "metadata": {},
   "source": [
    "When you started this talk, you had documents. \n",
    "\n",
    "Maybe you had metadata: filenames, dates, categories. \n",
    "\n",
    "But you didn't really know your data.\n",
    "\n",
    "Now? You can see it.\n",
    "\n",
    "You can see how documents cluster. \n",
    "\n",
    "You can find the duplicates inflating your dataset. \n",
    "\n",
    "You can discover connections between documents that keywords would miss. \n",
    "\n",
    "You can identify the prototypical examples and the edge cases. \n",
    "\n",
    "You can search for documents with similar diagrams, similar table structures, similar visual patterns.\n",
    "\n",
    "You transformed from 'I have documents' to 'I understand my dataset.'\n",
    "\n",
    "You've seen what's possible. How do you actually start using this on your own documents?\n",
    "\n",
    "The workflow is simple. \n",
    "\n",
    "Four steps:\n",
    "\n",
    "One: Embed. Load your documents, pick a model, compute embeddings. BiModernVBERT is a great starting point because it runs on CPU and is fast enough for most use cases.\n",
    "\n",
    "Two: Visualize. Generate a UMAP plot and look at your data. What clusters form? Where are the outliers? This 30-second view tells you more than hours of manual sampling.\n",
    "\n",
    "Three: Explore. Use similarity search, uniqueness, representativeness - whatever insights you need. Find duplicates. Discover similar documents. Identify prototypes.\n",
    "\n",
    "Four: Understand. You now know what you have, what you're missing, and what's unusual. You can make informed decisions about what to annotate, what to use for training, what to use for testing.\n",
    "\n",
    "Take 100 documents from your current project. Run this code. Look at the visualization. I guarantee you'll see something you didn't know about your dataset:\n",
    "\n",
    "- Clusters you didn't expect\n",
    "- Outliers that surprise you\n",
    "- Duplicates you didn't know existed\n",
    "- Connections keywords can't find\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
